{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "284ba83d-b0dc-4f6e-b49c-56b9d6c0de6b",
   "metadata": {},
   "source": [
    "### Step 1: Importing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5aaaf25-374f-4be2-a85d-e53fa85d35ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 13:45:05.578988: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-02 13:45:05.778032: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-02 13:45:05.779423: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-02 13:45:06.805868: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package stopwords to /home/yuvraj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/yuvraj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download NLTK resources (stopwords and WordNet)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Loading the pre-trained model\n",
    "W2V_model = KeyedVectors.load_word2vec_format('../Datasets/archive/GoogleNews-vectors-negative300.bin.gz',binary=True,limit=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5b34c6-5f85-46b4-b443-b3ea3fcfb4b6",
   "metadata": {},
   "source": [
    "### Step 2: Creation of Questions dataframe\n",
    "Currently there are 5 different files belonging to 5 different domain of the questions and in this module we will be creating a dataframe that will be having total of 1600 questions belonging to 5 different domains. The domains are \n",
    "- DSA questions\n",
    "- System design questions\n",
    "- AI questions\n",
    "- Computer fundamental questions\n",
    "- Behavioural questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2b174c-9b54-48d2-a322-a6e6d64f741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['../Dataset/DSA_que.txt', '../Dataset/System_Design_que.txt', '../Dataset/Behavioural_que.txt',\n",
    "              '../Dataset/CS_fundamentals.txt', '../Dataset/AI_que.txt']\n",
    "que_type = {0: 'DSA',\n",
    "            1: 'System_design',\n",
    "            2: 'Behavioural',\n",
    "            3: 'CS_fundamentals',\n",
    "            4: 'AI'}\n",
    "count = 0\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Going over all the paths\n",
    "for paths in file_paths:\n",
    "    try:\n",
    "        # Reset que_ls for each file iteration\n",
    "        que_ls = []\n",
    "\n",
    "        with open(paths, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                \n",
    "                # Removing the leading and following white space after reading content from the file\n",
    "                line = line.strip()\n",
    "                \n",
    "                # Saving the line in the que list\n",
    "                que_ls.append(line)\n",
    "\n",
    "            # Creating series from the \n",
    "            que_sr = pd.Series(que_ls)\n",
    "            temp_df = pd.DataFrame({'Que': que_sr})\n",
    "\n",
    "            # Adding a feature 'Category'\n",
    "            temp_df['Category'] = que_type[count]\n",
    "            count = count + 1\n",
    "\n",
    "            # Concatenating the dataframes\n",
    "            df = pd.concat([df, temp_df], axis=0)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {paths} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9c8ec8-6002-4c67-98f9-9e1a38afcc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the dataframe\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Resetting the index\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9308f7-8ff0-4922-9699-cb41c0e2f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe\n",
    "# df.to_csv('../Dataset/Que_Classification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f655b542-797e-4659-9d54-45e393c2fd89",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97813cf6-86e9-4261-9656-f4ec61f534f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4548186a-8ca5-44b9-872f-40def0f6e713",
   "metadata": {},
   "source": [
    "### Loading data to tensorflow dataset\n",
    "Loading data into tf.data.Dataset offers several advantages and reasons why it's commonly used in TensorFlow pipelines:\n",
    "\n",
    "- Efficient Memory Usage: TensorFlow dataset API provides efficient handling of large datasets by streaming data from disk or memory. It loads data on-the-fly, which is particularly useful when dealing with datasets that do not fit entirely into memory.\n",
    "\n",
    "- Parallelism: TensorFlow's data API supports parallel data loading and preprocessing. This enables faster data pipeline execution, especially on multi-core CPUs or GPUs, as data loading and preprocessing can be performed in parallel with model training or inference.\n",
    "\n",
    "- Data Transformation: tf.data.Dataset allows for easy and flexible data transformation and preprocessing. You can apply various transformations such as shuffling, batching, mapping, filtering, and more to the dataset to prepare it for training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f221d2-8f29-4f9e-a88a-27f29020ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch the dataset\n",
    "batch_size = 32\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eecee129-1f6a-4efe-86f4-dfaea5c186d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating TensorFlow Dataset directly from the DataFrame\n",
    "raw_text_ds = tf.data.Dataset.from_tensor_slices((df['Que']))\n",
    "\n",
    "# SHuffling the items and creating batches\n",
    "raw_text_ds = raw_text_ds.shuffle(len(df), seed=seed).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5867dcfd-a496-4c46-af39-c982327041c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************ Summary ************\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba5a9dc40864f4a8c949f1985d3ba6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples/sec (First included) 1512.66 ex/sec (total: 53 ex, 0.04 sec)\n",
      "Examples/sec (First only) 36.51 ex/sec (total: 1 ex, 0.03 sec)\n",
      "Examples/sec (First excluded) 6800.10 ex/sec (total: 52 ex, 0.01 sec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>BenchmarkResult:</strong><br/><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>first+lasts</th>\n",
       "      <td>0.035038</td>\n",
       "      <td>53</td>\n",
       "      <td>1512.662239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.027391</td>\n",
       "      <td>1</td>\n",
       "      <td>36.508854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasts</th>\n",
       "      <td>0.007647</td>\n",
       "      <td>52</td>\n",
       "      <td>6800.098549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "BenchmarkResult(stats=             duration  num_examples          avg\n",
       "first+lasts  0.035038            53  1512.662239\n",
       "first        0.027391             1    36.508854\n",
       "lasts        0.007647            52  6800.098549, raw_stats=                    duration\n",
       "start_time        853.755104\n",
       "first_batch_time  853.782494\n",
       "end_time          853.790141\n",
       "num_iter           52.000000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display some benchmark statistics\n",
    "tfds.benchmark(raw_text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b68f1645-fd3b-4aff-850a-f900af3605ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Que b'What is the purpose of the I/O scheduler in an operating system?'\n",
      "Que b'Describe a situation where you had to collaborate with other departments to enhance customer experience.'\n",
      "Que b'What is the difference between breadth-first search (BFS) and depth-first search (DFS)?'\n",
      "Que b'How would you design an e-commerce checkout system?'\n",
      "Que b'Discuss the role of activation functions in preventing vanishing gradients.'\n"
     ]
    }
   ],
   "source": [
    "for text_batch in raw_text_ds.take(1):\n",
    "  for i in range(5):\n",
    "    print(\"Que\", text_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77389273-1049-4c98-acce-68899902cd3d",
   "metadata": {},
   "source": [
    "### Train, test split + Data loading optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c30f8f47-53bc-4b89-a814-0c068e0a5e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Batches :  52\n",
      "Total Training Batches (80:20) :  41.6\n",
      "Total Testing Batches :  10.4\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Batches : \",len(raw_text_ds))\n",
    "print(\"Total Training Batches (80:20) : \",len(raw_text_ds)*0.8)\n",
    "print(\"Total Testing Batches : \",len(raw_text_ds)*0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc3e67-5c39-43aa-ad73-cae01f088fe4",
   "metadata": {},
   "source": [
    "The performance of a dataset pipeline can have a significant impact on the performance of a machine learning model. If a dataset pipeline is slow, it can bottleneck the overall performance of the model. tfds.benchmark is a simple and easy-to-use tool for evaluating the performance of dataset pipelines. It can be used to identify bottlenecks, compare different pipelines, and track progress over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ed3b3a5-7684-4a0e-bebb-279cb33720c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_val(ds,train_size,val_size):\n",
    "    \n",
    "    # Calculating total batches\n",
    "    total_batches = len(ds)\n",
    "    \n",
    "    # Extracting training,testing and validation batch from the dataset (ds)\n",
    "    train_ds_batches = int(train_size*total_batches)\n",
    "    test_ds_batches = int(val_size*total_batches)\n",
    "    \n",
    "    # 80:20\n",
    "    train_ds = ds.take(train_ds_batches) \n",
    "    test_ds = ds.skip(train_ds_batches).take(test_ds_batches)\n",
    "    \n",
    "    # Catching and prefetching the dataset to improve data pipeline performance\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_ds,test_ds\n",
    "\n",
    "\n",
    "# Calling the function\n",
    "train_ds,test_ds = create_train_test_val(raw_text_ds,0.8,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4e9ba63-0401-4ee4-a5bb-2c1128b30e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************ Summary ************\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501d7c476cd5478f845844c6af2a3431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples/sec (First included) 1728.01 ex/sec (total: 42 ex, 0.02 sec)\n",
      "Examples/sec (First only) 56.28 ex/sec (total: 1 ex, 0.02 sec)\n",
      "Examples/sec (First excluded) 6271.10 ex/sec (total: 41 ex, 0.01 sec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>BenchmarkResult:</strong><br/><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>first+lasts</th>\n",
       "      <td>0.024305</td>\n",
       "      <td>42</td>\n",
       "      <td>1728.008144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.017768</td>\n",
       "      <td>1</td>\n",
       "      <td>56.282497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasts</th>\n",
       "      <td>0.006538</td>\n",
       "      <td>41</td>\n",
       "      <td>6271.099957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "BenchmarkResult(stats=             duration  num_examples          avg\n",
       "first+lasts  0.024305            42  1728.008144\n",
       "first        0.017768             1    56.282497\n",
       "lasts        0.006538            41  6271.099957, raw_stats=                    duration\n",
       "start_time        853.835985\n",
       "first_batch_time  853.853752\n",
       "end_time          853.860290\n",
       "num_iter           41.000000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display some benchmark statistics\n",
    "tfds.benchmark(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a8fac-c641-459c-8f82-98a581ee5dac",
   "metadata": {},
   "source": [
    "### Data processing pipeline\n",
    "\n",
    "cat and Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ad32306-8c87-4032-99e4-6c0d46e2d9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differnt\n",
      "Same\n"
     ]
    }
   ],
   "source": [
    "# Example to understand why we need to do lowercasing\n",
    "word1 = \"cat\"\n",
    "word2 = \"Cat\"\n",
    "word2_lowercased = word2.lower()\n",
    "\n",
    "def compare(word1,word2):\n",
    "    if word1 == word2:\n",
    "        print(\"Same\")\n",
    "    else:\n",
    "        print(\"Differnt\")\n",
    "\n",
    "compare(word1,word2)\n",
    "compare(word1,word2_lowercased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eca0014-cc2a-457c-a608-121c5ea06dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single text entry in the dataset\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Input : Single raw text\n",
    "    Output: Cleaned text\n",
    "\n",
    "    Description: This function will take a single raw text as input, remove all stopwords and punctuation, then lowercase the words to eliminate any ambiguity. \n",
    "    Ultimately, lemmatization will be applied to the text, and clean text will be returned.\n",
    "    \"\"\"\n",
    "    # Lowercasing and Tokenizing the text\n",
    "    tokens = tf.strings.lower(tf.strings.split(text))\n",
    "    \n",
    "    # Removing the stopwords as they don't provide any information\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.numpy().decode('utf-8') not in stop_words]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [tf.strings.regex_replace(token, '[%s]' % re.escape(string.punctuation), '') for token in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token.numpy().decode('utf-8')) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    processed_text = tf.strings.reduce_join(tokens, separator=' ')\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "507aba75-63b9-48d5-82a5-47919772c5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'quick brown fox jump lazy dog seem care it'\n"
     ]
    }
   ],
   "source": [
    "str = \"The quick brown fox jumps over the lazy dogs, but they don't seem to care about it.\"\n",
    "output = process_text(str)\n",
    "print(output.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8be994-8def-4388-a693-af8223bfdb23",
   "metadata": {},
   "source": [
    "The tf.py_function is used in this context to incorporate a Python function (process_text in this case) into a TensorFlow computational graph. TensorFlow operations are typically written in TensorFlow's native language (graph operations) for better performance, but sometimes you need to use external Python code that TensorFlow doesn't know how to execute directly. tf.py_function serves as a bridge to allow you to use regular Python functions within the TensorFlow graph.\n",
    "\n",
    "Here's why you need tf.py_function in the process_dataset_element:\n",
    "\n",
    "- Integration with TensorFlow Graph: TensorFlow operates with a computational graph, and many of its operations are written in C++ or CUDA for efficiency. The tf.py_function allows you to include your custom Python code (process_text) within this graph, ensuring that the entire data processing pipeline can be efficiently executed.\n",
    "\n",
    "- Eager Execution Compatibility: If you're working in TensorFlow 2.0 or later with eager execution enabled, you might wonder why you need tf.py_function. While eager execution is more Pythonic and allows you to use regular Python functions directly, using tf.py_function ensures that the same code can be seamlessly switched to graph mode for performance gains during model training.\n",
    "\n",
    "- Parallel Execution and Distributed Training: TensorFlow can parallelize the execution of operations, and tf.py_function allows TensorFlow to handle parallel execution efficiently. This is particularly important when dealing with large datasets or when training models on distributed systems.\n",
    "\n",
    "- Consistency in Tensor Shapes: When using tf.py_function, TensorFlow is better able to manage and infer the shapes of the tensors involved, ensuring compatibility with the rest of your graph.\n",
    "\n",
    "In summary, tf.py_function is a wrapper that allows you to use your custom Python functions within the TensorFlow graph, ensuring compatibility with TensorFlow's computation engine. While it introduces a slight overhead due to the transition between Python and TensorFlow execution, it is a necessary step to incorporate non-TensorFlow code seamlessly into the TensorFlow pipeline. If you are using TensorFlow 2.0 or later, you can experiment without tf.py_function and rely on eager execution, but keep in mind the potential performance implications, especially during large-scale training scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15432914-b61b-46ce-9802-fd029fce48aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to process a single element of the dataset\n",
    "def process_dataset_element(element):\n",
    "\n",
    "    processed_text = tf.py_function(func=process_text, inp=[element], Tout=tf.string)\n",
    "    return processed_text\n",
    "\n",
    "# Assuming you have a train_ds dataset\n",
    "cleaned_train_ds = train_ds.map(process_dataset_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7214d865-072d-4383-bdca-8e6baced0ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "016dc1ea-7c6d-4bcc-bc6c-c701cf749712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'quick brown fox jump lazy dog seem care it'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67569dbb-8f59-4d03-8d4c-0ca175ec6c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
